1.Расскажите, как работает регуляризация в решающих деревьях, какие параметры мы штрафуем в данных алгоритмах?
Использование регуляризации – это то же самое, что и название. Регуляризация означает сделать вещи приемлемыми или регулярными. 
Регуляризация – это техника, которая уменьшает ошибки модели, избегая переоснащения и обучая модель правильному функционированию.
Основная причина “переподгонки” модели заключается в том, что она не может обобщить данные из-за слишком большой нерелевантности.
 Однако регуляризация является эффективным методом, повышающим точность модели и уменьшающим ненужные отклонения.
Кроме того, этот метод позволяет избежать потери важных данных, что происходит с подгонкой. Регуляризация помогает модели учиться,
 применяя ранее изученные примеры к новым невидимым данным. Также можно уменьшить емкость модели, сводя различные параметры к нулю.
1) Регуляризация это способ борьбы с переобучением.
По сути регуляризация является штрафом за слишком большие веса.
2)Без регуляризатора мы просто минимизируем функционал качества Q(w, X)->min
3) а с регуляризатором добавляется еще одно условие:
\\w\\**2<=C
т.е.решаем исходную задачу, но при этом ограничиваем по норме векторы весов
 Регуляризация удалит дополнительные веса из конкретных характеристик и равномерно распределит эти веса.
регуляризация: L1 и L2
Штрафуя модель, мы ограничиваем то количество решений, которое в случае с мультиколлинеарностью слишком велико или бесконечно, 
некоторым набором решений. Да, в результате мы получаем несколько смещённую оценку, но она смещена не сильно. 
Таким образом от глобальной проблемы (задачу решить в принципе нельзя) мы переходим к конкретному результату, 
когда задача решена не самым оптимальным способом, но, тем не менее, устойчивое решение мы нашли.
1) L1 - негладкий(модуль не имеет производной в 0)
т.е. оптимизация модели с таким регуляризатором будет затруднительна
- интересное св-во: если использовать L1, то часть весов в итоговом векторе весов будут нулевыми
Т.e. L1 производит отбор признаков-использует в модели не все признаки, а только самые важные из них
2) L2 -гладкий и выпуклый(его добавление к функционалу не будет усложнять процесс оптимизации)
Решающее дерево (decision tree, DT) — это логический алгоритм классификации, основанный на поиске конъюнктивных закономерностей
Деревья решений классификации — В деревьях такого типа переменная решения является категориальной.
Деревья решений регрессии. В деревьях решений такого типа переменная решения является непрерывной
Методы регуляризации решающих деревьев.
 Деревья легко переобучаются и процесс ветвления надо в какой-то момент останавливать.
Для этого есть разные критерии, обычно используются все сразу:
ограничение по максимальной глубине дерева;
ограничение на минимальное количество объектов в листе;
ограничение на максимальное количество листьев в дереве;
требование, чтобы функционал качества Branch при делении текущей подвыборки на две улучшался не менее чем на s процентов.
Делать это можно на разных этапах работы алгоритма, что не меняет сути, но имеет разные устоявшиеся названия:
можно проверять критерии прямо во время построения дерева, такой способ называется pre-pruning или early stopping;
а можно построить дерево жадно без ограничений, а затем провести стрижку (pruning), то есть удалить некоторые вершины из дерева так,
 чтобы итоговое качество упало не сильно, но дерево начало подходить под условия регуляризации. 
При этом качество стоит измерять на отдельной, отложенной выборке
XGBoos
Функционал регуляризуется 
добавляются штрафы за количество листьев иза норму коэффициентов.
2.По какому принципу рассчитывается "важность признака (feature_importance)" в ансамблях деревьев?
Важность рассчитывается для отдельного дерева решений по величине, на которую каждая точка разделения 
атрибута улучшает показатель производительности, взвешенный по количеству наблюдений, за которые отвечает узел. 
Показателем производительности может быть чистота (индекс Джини), используемая для выбора точек разделения, 
или другая более конкретная функция ошибок.



